{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import gaussian_kde\n",
    "import missingno as msno\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7639e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train.csv')\n",
    "test_data = pd.read_csv('./data/test.csv')\n",
    "gender_submission = pd.read_csv('./data/gender_submission.csv')\n",
    "gender_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b41d41",
   "metadata": {},
   "source": [
    "# Step 1.\n",
    "Predict all passengers as deceased (Survived = 0) <br>\n",
    "<img src=\"./images/step1submission.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fc3941",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": test_data[\"PassengerId\"],\n",
    "    \"Survived\": 0\n",
    "    })\n",
    "submission.to_csv('./submission/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab3a15f",
   "metadata": {},
   "source": [
    "# Step 2.\n",
    "Predict all females as survived and all males as deceased.\n",
    "<img src=\"./images/step2submission.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b0c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": test_data[\"PassengerId\"],\n",
    "    \"Survived\": test_data[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
    "    })\n",
    "submission.to_csv('./submission/step2submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d83fea",
   "metadata": {},
   "source": [
    "# Step 3. **EDA**\n",
    "\n",
    "Explore survival rates by __gender__, __age__, __passenger class (Pclass)__, and __family status\n",
    "(SibSp/Parch)__\n",
    "\n",
    "Tracks\n",
    "- PassengerId - 탑승객 Id\n",
    "- Survived - 생존유무 (0 = Deseased, 1 = Survived)\n",
    "- Pclass - 티켓 클래스 (1 = 1st, 2 = 2nd, 3 = 3rd)\n",
    "- Name - 탑승객 성명\n",
    "- Sex - 성별\n",
    "- Age - 나이(세)\n",
    "- SibSp - 함께 탑승한 형제자매, 배우자 수 총합\n",
    "- Parch - 함께 탑승한 부모, 자녀 수 총합\n",
    "- Ticket - 티켓 넘버\n",
    "- Fare - 탑승 요금\n",
    "- Cabin - 객실 넘버\n",
    "- Embarked - 탑승 항구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f468a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of train data\n",
    "print('Number of train data: ', len(train_data))\n",
    "# number of test data\n",
    "print('Number of test data: ', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f820586",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac5f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16013d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "gender_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbbd6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, test_data 결측치 확인\n",
    "train_null = train_data.isnull().sum()\n",
    "test_null = test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a4545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34816ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f97dfeb",
   "metadata": {},
   "source": [
    "train_data missing values: Age, Cabin, Embarked\n",
    "\n",
    "test_data missing values: Age, Fare, Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfbacf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values visualization\n",
    "msno.matrix(train_data, figsize=(12,5))\n",
    "msno.matrix(test_data, figsize=(12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861189f",
   "metadata": {},
   "source": [
    "Overall Survival Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a1238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Survival Rate\n",
    "train_data['Survived'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "labels = ['Deseased', 'Survived']\n",
    "colors = ['#1B4F72', '#AED6F1']\n",
    "\n",
    "train_data['Survived'].value_counts().plot.pie(explode=[0, 0.08],\n",
    "                                              autopct='%1.1f%%',\n",
    "                                              labels=labels,\n",
    "                                              textprops={'fontsize': 15},\n",
    "                                              colors=colors)\n",
    "plt.ylabel('Survival Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db19487",
   "metadata": {},
   "source": [
    "Survival Count / Rates by Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb7046",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "axes[0].set_title(\"Survival Count by Gender\", size=15)\n",
    "sns.countplot(x=\"Sex\", hue=\"Survived\", data=train_data, ax=axes[0], palette=['#1B4F72', '#AED6F1'], order=['male', 'female'])\n",
    "axes[0].legend(labels=['Deceased', 'Survived'])\n",
    "\n",
    "# Right - Survival Rates\n",
    "gender_survival_rate = train_data.groupby('Sex')['Survived'].value_counts(normalize=True).reset_index(name='proportion')\n",
    "\n",
    "sns.barplot(x='Sex', y='proportion', hue='Survived', data=gender_survival_rate, ax=axes[1], order=['male', 'female'], palette=['#1B4F72', '#AED6F1'])\n",
    "\n",
    "for p in axes[1].patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:\n",
    "        axes[1].text(p.get_x() + p.get_width() / 2.,\n",
    "                     height + 0.01,\n",
    "                     f'{height:.1%}',\n",
    "                     ha=\"center\",\n",
    "                     fontsize=12)\n",
    "\n",
    "axes[1].set_title('Survival Rate by Gender', fontsize=15)\n",
    "axes[1].set_ylabel('Rate', fontsize=12)\n",
    "axes[1].set_xlabel('Sex', fontsize=12)\n",
    "axes[1].set_ylim(0, 1.0)\n",
    "handles, labels = axes[1].get_legend_handles_labels()\n",
    "axes[1].legend(handles, ['Deceased', 'Survived'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec6f40f",
   "metadata": {},
   "source": [
    "Survival Count / Rates by Age Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009eceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd34cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(train_data['Age'], bins=25, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4f8807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival Count by Age Group\n",
    "train_data['AgeGroup'] = pd.cut(train_data['Age'], bins=[0, 12, 20, 40, 60, 80], labels=['Child', 'Teenager', 'Adult', 'Middle Aged', 'Senior'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(x='AgeGroup', hue='Survived', data=train_data, palette=['#1B4F72', '#AED6F1'])\n",
    "ax.set_title('Survival Count by Age Group', fontsize=15)\n",
    "ax.legend(labels=['Deceased', 'Survived'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d6ec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival Rates by Age Group\n",
    "train_data['AgeGroup'] = pd.cut(train_data['Age'], bins=[0, 12, 20, 40, 60, 80], labels=['Child', 'Teenager', 'Adult', 'Middle Aged', 'Senior'])\n",
    "\n",
    "age_survival_rate = train_data.groupby('AgeGroup')['Survived'].value_counts(normalize=True).reset_index(name='proportion')\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "ax = sns.barplot(x='AgeGroup', y='proportion', hue='Survived', data=age_survival_rate, palette=['#1B4F72', '#AED6F1'])\n",
    "\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:\n",
    "        ax.text(p.get_x() + p.get_width() / 2.,\n",
    "                height + 0.01,\n",
    "                f'{height:.1%}',\n",
    "                ha=\"center\",\n",
    "                fontsize=12)\n",
    "\n",
    "ax.set_title('Survival Rate by Age Group', fontsize=15)\n",
    "ax.set_ylabel('Rate')\n",
    "ax.set_ylim(0, 1.0)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles, labels=['Deceased', 'Survived'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321aca49",
   "metadata": {},
   "source": [
    "Survival info. by PClass (Ticket Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba271608",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['Pclass', 'Survived']].groupby('Pclass').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c20ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total passengers of each PClass\n",
    "plt.figure(figsize=(5, 3))\n",
    "ax = sns.countplot(data=train_data, x='Pclass',\n",
    "                   palette=['#1B4F72', '#5DADE2', '#AED6F1'])\n",
    "\n",
    "ax.set_title(\"PClass Passengers Count\", size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival Count / Rates by PClass\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Left - Survival Count\n",
    "axes[0].set_title(\"Survival Count by PClass\", size=15)\n",
    "sns.countplot(x=\"Pclass\", hue=\"Survived\", data=train_data, ax=axes[0], palette=['#1B4F72', '#AED6F1'])\n",
    "axes[0].legend(labels=['Deceased', 'Survived'])\n",
    "\n",
    "# Right - Survival Rates\n",
    "pclass_survival_rate = train_data.groupby('Pclass')['Survived'].value_counts(normalize=True).reset_index(name='proportion')\n",
    "sns.barplot(x='Pclass', y='proportion', hue='Survived', data=pclass_survival_rate, ax=axes[1], palette=['#1B4F72', '#AED6F1'])\n",
    "\n",
    "for p in axes[1].patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:\n",
    "        axes[1].text(p.get_x() + p.get_width() / 2.,\n",
    "                     height + 0.01,\n",
    "                     f'{height:.1%}',\n",
    "                     ha=\"center\",\n",
    "                     fontsize=13)\n",
    "\n",
    "axes[1].set_title(\"Survival Rate by PClass\", size=15)\n",
    "axes[1].set_ylabel(\"Rate\")\n",
    "axes[1].set_ylim(0, 1.0)\n",
    "handles, labels = axes[1].get_legend_handles_labels()\n",
    "axes[1].legend(handles=handles, labels=['Deceased', 'Survived'])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08feb496",
   "metadata": {},
   "source": [
    "Fare distribution by Survival Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fare distribution by Survival Status\n",
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "sns.kdeplot(train_data[train_data['Survived']==0]['Fare'], ax=ax, color='#610085', label='Deceased')\n",
    "sns.kdeplot(train_data[train_data['Survived']==1]['Fare'], ax=ax, color='#197EC2', label='Survived')\n",
    "\n",
    "ax.set(xlim=(0, train_data['Fare'].max()))\n",
    "ax.set_title('Fare Distribution by Survival Status', fontsize=15)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b568ad1",
   "metadata": {},
   "source": [
    "Survival Count / Rates by FamilyStatus (SibSp/Parch)\n",
    "\n",
    " - SibSp - 함께 탑승한 형제자매, 배우자 수 총합\n",
    " - Parch - 함께 탑승한 부모, 자녀 수 총합  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b663e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['SibSp', 'Survived']].groupby(['SibSp']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5654065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['Parch', 'Survived']].groupby(['Parch']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f2d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival Count by SibSp\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Left - Total Passengers by SibSp\n",
    "axes[0].set_title(\"SibSp Passengers Count\", size=15)\n",
    "sns.countplot(x=\"SibSp\", data=train_data, ax=axes[0], palette=[\"#A4E8F7\", \"#65ADDD\", \"#387FAF\", \"#045185\", '#1B4F72', '#1B4F72', '#1B4F72'])\n",
    "\n",
    "# Right - Survival Count by SibSp\n",
    "axes[1].set_title(\"Survival Count by SibSp\", size=15)\n",
    "sns.countplot(x=\"SibSp\", hue=\"Survived\", data=train_data, ax=axes[1], palette=['#1B4F72', '#AED6F1'])\n",
    "handles, labels = axes[1].get_legend_handles_labels()\n",
    "axes[1].legend(handles=handles, labels=['Deceased', 'Survived'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b142bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival Count by Parch\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Left - Total Parch Passengers\n",
    "axes[0].set_title(\"Parch Passengers Count\", size=15)\n",
    "sns.countplot(x=\"Parch\", data=train_data, ax=axes[0], palette=[\"#A4E8F7\", \"#65ADDD\", \"#387FAF\", \"#045185\", '#1B4F72', '#1B4F72', '#1B4F72'])\n",
    "\n",
    "# Right - Survival Count by Parch\n",
    "axes[1].set_title(\"Survival Count by Parch\", size=15)\n",
    "sns.countplot(x=\"Parch\", hue=\"Survived\", data=train_data, ax=axes[1], palette=['#1B4F72', '#AED6F1'])\n",
    "handles, labels = axes[1].get_legend_handles_labels()\n",
    "axes[1].legend(handles=handles, labels=['Deceased', 'Survived'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08189637",
   "metadata": {},
   "source": [
    "# Step 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5507e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train.csv')\n",
    "test_data = pd.read_csv('./data/test.csv')\n",
    "\n",
    "dataset = pd.concat([train_data, test_data], sort=False).reset_index(drop=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967a966f",
   "metadata": {},
   "source": [
    "### Embarked null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9c1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6081ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['Embarked'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92009652",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Embarked'].fillna('S', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b0311b",
   "metadata": {},
   "source": [
    "### Fare null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230fb4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset['Fare'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93068566",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Fare'].groupby(dataset['Pclass']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77721c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[1043, 'Fare'] = 13.3028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c26f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Age'] = dataset['Age'].groupby([dataset['Pclass'], dataset['Sex']]).transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761de9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['LastName'] = dataset['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\n",
    "dataset['Survived'].groupby(dataset['LastName']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff826e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['LastName'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee051cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['LastName'] = dataset['LastName'].replace(['Capt', 'Col', 'Dr', 'Major', 'Rev', 'Don', 'Sir', 'Jonkheer'], 'Mr')\n",
    "dataset['LastName'] = dataset['LastName'].replace(['Ms', 'Mlle'], 'Miss')\n",
    "dataset['LastName'] = dataset['LastName'].replace(['Mme', 'Lady', 'Countess', 'Dona'], 'Mrs')\n",
    "dataset['LastName'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df5e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch']\n",
    "dataset['TicketFreq'] = dataset.groupby('Ticket')['Ticket'].transform('count')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da615d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[dataset['FamilySize'] == 0, 'Solo'] = 1\n",
    "dataset.loc[dataset['TicketFreq'] == 1, 'Solo'] = 1\n",
    "dataset['Solo'] = dataset['Solo'].fillna(0)\n",
    "\n",
    "dataset['Fare'] = pd.qcut(dataset['Fare'], 9, labels=[1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "dataset['Age'] = pd.qcut(dataset['Age'], 10, labels=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "dataset = pd.concat([dataset, pd.get_dummies(dataset['Sex'])], axis=1)\n",
    "dataset.rename(columns={'male': 'Male', 'female':'Female'}, inplace=True)\n",
    "\n",
    "dataset = pd.concat([dataset, pd.get_dummies(dataset['Embarked'], prefix='Embarked')], axis=1)\n",
    "\n",
    "dataset = pd.concat([dataset, pd.get_dummies(dataset['LastName'])], axis=1)\n",
    "\n",
    "dataset = dataset.drop(columns=['Name', 'Sex', 'Ticket', 'Cabin', 'SibSp', 'Parch', 'Embarked', 'LastName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69309b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['FamilySize'] = pd.cut(dataset['FamilySize'], bins=[-1, 0, 1, 4, 10], labels=[0, 1, 2, 3])\n",
    "dataset['Survived'].groupby(dataset['TicketFreq']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d78d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['TicketFreq'] = pd.cut(dataset['TicketFreq'], bins=[0, 1, 2, 4, 20], labels=[0, 1, 2, 3])\n",
    "dataset['TicketFreq'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bc57a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b272c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "# PassengerId 제외하고 정규화\n",
    "dataset[list(dataset.columns.difference(['PassengerId']))] = scaler.fit_transform(dataset[list(dataset.columns.difference(['PassengerId']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset.loc[:890, 'Survived']\n",
    "dataset = dataset.drop(columns = 'Survived')\n",
    "features = dataset.iloc[:891, :]\n",
    "features.drop('PassengerId', axis=1, inplace=True)\n",
    "test_dataset = dataset.iloc[891:, :]\n",
    "feature_names = features.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa08b09",
   "metadata": {},
   "source": [
    "# Step 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d55d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fecc7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(features), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58a6da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2aea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Logistic Regression Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Logistic Regression Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cdc2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset['PassengerId'] = test_dataset['PassengerId'].astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": test_dataset[\"PassengerId\"],\n",
    "    \"Survived\": lr.predict(test_dataset.drop('PassengerId', axis=1)).astype(int)\n",
    "    })\n",
    "submission.to_csv('./submission/lr_liblinear_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e9d8cf",
   "metadata": {},
   "source": [
    "<img src=\"./images/lr_liblinear_submission.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde14da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LogisticRegression(solver='lbfgs')\n",
    "lr2.fit(X_train, y_train)\n",
    "y_pred = lr2.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Logistic Regression Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Logistic Regression Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56baf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset['PassengerId'] = test_dataset['PassengerId'].astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": test_dataset[\"PassengerId\"],\n",
    "    \"Survived\": lr2.predict(test_dataset.drop('PassengerId', axis=1)).astype(int)\n",
    "    })\n",
    "submission.to_csv('./submission/lr_libfgs_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7685c6",
   "metadata": {},
   "source": [
    "<img src=\"./images/lr_libfgs_submission.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c4bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr3 = LogisticRegression(solver='saga')\n",
    "lr3.fit(X_train, y_train)\n",
    "y_pred = lr3.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Logistic Regression Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Logistic Regression Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddd350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset['PassengerId'] = test_dataset['PassengerId'].astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": test_dataset[\"PassengerId\"],\n",
    "    \"Survived\": lr3.predict(test_dataset.drop('PassengerId', axis=1)).astype(int)\n",
    "    })\n",
    "submission.to_csv('./submission/lr_saga_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c5c1a1",
   "metadata": {},
   "source": [
    "<img src=\"./images/lr_saga_submission.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2839cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "멀티코어 활용 + tqdm 오류 수정(단일 셀)\n",
    "- tqdm 래퍼가 iterable=None도 지원: with TQDM(total=...) as bar: 형태와 TQDM(iterable, total=...) 둘 다 동작\n",
    "- DT/SVM 탐색을 ProcessPoolExecutor로 병렬화, RF/BO는 n_jobs=CPU 활용, MLP는 GPU 단일 프로세스\n",
    "- 검증 정확도는 predict() 기준(일관성), 앙상블 확률 수집 후 soft voting\n",
    "\"\"\"\n",
    "\n",
    "# ===== 멀티코어/스레드 환경 변수 =====\n",
    "import os\n",
    "CPU = os.cpu_count() or 1\n",
    "for k in [\"OMP_NUM_THREADS\",\"OPENBLAS_NUM_THREADS\",\"MKL_NUM_THREADS\",\"VECLIB_MAXIMUM_THREADS\",\"NUMEXPR_NUM_THREADS\"]:\n",
    "    os.environ[k] = str(CPU)\n",
    "\n",
    "import sys, time, math, random, warnings, itertools, io\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# tqdm 고정: iterable=None도 허용 (수정 포인트)\n",
    "from tqdm.std import tqdm as _tqdm\n",
    "def TQDM(iterable=None, **kw):\n",
    "    cfg = dict(file=sys.stdout, ncols=90, mininterval=0.3, leave=True, dynamic_ncols=False)\n",
    "    if iterable is None:\n",
    "        return _tqdm(**{**cfg, **kw})\n",
    "    return _tqdm(iterable, **{**cfg, **kw})\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# ===== 설정 =====\n",
    "SEED = 42\n",
    "\n",
    "# Decision Tree (Random Search, 병렬)\n",
    "N_DT_SAMPLES = 120\n",
    "DT_MAX_DEPTH_RANGE = (2, 40)\n",
    "DT_MIN_SPLIT_RANGE = (2, 40)\n",
    "DT_MIN_LEAF_RANGE = (1, 20)\n",
    "DT_CRITERIA = [\"gini\", \"entropy\", \"log_loss\"]\n",
    "DT_SPLITTERS = [\"best\", \"random\"]\n",
    "DT_USE_NONE_DEPTH_PROB = 0.15\n",
    "\n",
    "# Random Forest (Random Search, 내부 멀티코어 사용)\n",
    "N_RF_SAMPLES = 160\n",
    "RF_N_EST_RANGE = (100, 2000)\n",
    "RF_MAX_DEPTH_RANGE = (2, 50)\n",
    "RF_MIN_SPLIT_RANGE = (2, 30)\n",
    "RF_MIN_LEAF_RANGE = (1, 20)\n",
    "RF_MAX_FEATURES_CHOICES = [\"sqrt\", \"log2\", None, \"float\"]\n",
    "RF_MAX_FEATURES_FLOAT_RANGE = (0.2, 1.0)\n",
    "RF_BOOTSTRAP_CHOICES = [True, False]\n",
    "RF_CLASS_WEIGHT_CHOICES = [None, \"balanced\"]\n",
    "\n",
    "# SVM (Random Search, 병렬; 탐색 probability=False, 최종만 True)\n",
    "N_SVM_SAMPLES_PER_KERNEL = 100\n",
    "SVM_KERNELS = [\"rbf\", \"linear\", \"poly\", \"sigmoid\"]\n",
    "SVM_C_LOG_RANGE = (-3, 3)\n",
    "SVM_GAMMA_LOG_RANGE = (-4, 1)\n",
    "SVM_DEGREE_RANGE = (2, 5)\n",
    "SVM_COEF0_RANGE = (-1.0, 1.0)\n",
    "\n",
    "# BO for RF (수동 GP+EI; 순차, 내부 RF는 n_jobs=CPU)\n",
    "BO_INIT_SAMPLES = 12\n",
    "BO_MAX_ITERS = 40\n",
    "BO_EARLY_STOP_PATIENCE = 10\n",
    "BO_IMPROVEMENT_TOL = 1e-4\n",
    "BO_N_CANDIDATES = 800\n",
    "BO_BOUNDS_LO = np.array([100,  2,  2, 0.2], dtype=float)  # (n_estimators, max_depth, min_samples_split, max_features_float)\n",
    "BO_BOUNDS_HI = np.array([2000, 50, 30, 1.0], dtype=float)\n",
    "\n",
    "# MLP (Random Search; GPU 단일 프로세스)\n",
    "RUN_MLP = True\n",
    "N_MLP_SAMPLES = 16\n",
    "MLP_HIDDEN_RANGE = (64, 512)\n",
    "MLP_DROPOUT_RANGE = (0.0, 0.5)\n",
    "MLP_LR_LOG_RANGE = (-4, -2.5)\n",
    "MLP_EPOCHS_RANGE = (120, 220)\n",
    "MLP_BATCH_CHOICES = [32, 64, 128]\n",
    "MLP_EARLY_STOP_PATIENCE = 20\n",
    "\n",
    "# 앙상블 가중치 (None=균등)\n",
    "ENSEMBLE_WEIGHTS = None\n",
    "\n",
    "# ===== sklearn / torch =====\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as CK, WhiteKernel\n",
    "from scipy.stats import norm\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    TORCH_OK = True\n",
    "except Exception:\n",
    "    TORCH_OK = False\n",
    "\n",
    "# ===== 재현성 =====\n",
    "np.random.seed(SEED); random.seed(SEED)\n",
    "if TORCH_OK:\n",
    "    try:\n",
    "        torch.manual_seed(SEED)\n",
    "        if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "    except Exception: pass\n",
    "\n",
    "os.makedirs(\"./submission\", exist_ok=True)\n",
    "\n",
    "# ===== 데이터/피처 =====\n",
    "def build_features_from_raw():\n",
    "    train_data = pd.read_csv('./data/train.csv')\n",
    "    test_data  = pd.read_csv('./data/test.csv')\n",
    "\n",
    "    dataset = pd.concat([train_data, test_data], sort=False).reset_index(drop=True)\n",
    "\n",
    "    dataset['Embarked'].fillna('S', inplace=True)\n",
    "    if dataset['Fare'].isnull().any():\n",
    "        if 1043 in dataset.index: dataset.loc[1043, 'Fare'] = 13.3028\n",
    "        dataset['Fare'] = dataset['Fare'].fillna(dataset.groupby('Pclass')['Fare'].transform('mean'))\n",
    "\n",
    "    dataset['Age'] = dataset['Age'].groupby([dataset['Pclass'], dataset['Sex']]).transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "    dataset['LastName'] = dataset['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\n",
    "    dataset['LastName'] = dataset['LastName'].replace(['Capt','Col','Dr','Major','Rev','Don','Sir','Jonkheer'],'Mr')\n",
    "    dataset['LastName'] = dataset['LastName'].replace(['Ms','Mlle'],'Miss')\n",
    "    dataset['LastName'] = dataset['LastName'].replace(['Mme','Lady','Countess','Dona'],'Mrs')\n",
    "\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch']\n",
    "    dataset['TicketFreq'] = dataset.groupby('Ticket')['Ticket'].transform('count')\n",
    "    dataset.loc[dataset['FamilySize'] == 0, 'Solo'] = 1\n",
    "    dataset.loc[dataset['TicketFreq'] == 1, 'Solo'] = 1\n",
    "    dataset['Solo'] = dataset['Solo'].fillna(0)\n",
    "\n",
    "    dataset['Fare'] = pd.qcut(dataset['Fare'], 9, labels=[1,2,3,4,5,6,7,8,9])\n",
    "    dataset['Age']  = pd.qcut(dataset['Age'], 10, labels=[1,2,3,4,5,6,7,8,9,10])\n",
    "\n",
    "    dataset = pd.concat([dataset, pd.get_dummies(dataset['Sex'])], axis=1)\n",
    "    dataset.rename(columns={'male':'Male','female':'Female'}, inplace=True)\n",
    "    dataset = pd.concat([dataset, pd.get_dummies(dataset['Embarked'], prefix='Embarked')], axis=1)\n",
    "    dataset = pd.concat([dataset, pd.get_dummies(dataset['LastName'])], axis=1)\n",
    "\n",
    "    dataset = dataset.drop(columns=['Name','Sex','Ticket','Cabin','SibSp','Parch','Embarked','LastName'])\n",
    "\n",
    "    dataset['FamilySize'] = pd.cut(dataset['FamilySize'], bins=[-1,0,1,4,10], labels=[0,1,2,3])\n",
    "    dataset['TicketFreq'] = pd.cut(dataset['TicketFreq'], bins=[0,1,2,4,20], labels=[0,1,2,3])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scale_cols = list(dataset.columns.difference(['PassengerId','Survived']))\n",
    "    dataset[scale_cols] = scaler.fit_transform(dataset[scale_cols])\n",
    "\n",
    "    labels   = dataset.loc[:890, 'Survived']\n",
    "    features = dataset.iloc[:891, :].drop(columns='Survived')\n",
    "    test_ds  = dataset.iloc[891:, :].copy()\n",
    "    test_ds  = test_ds.drop(columns='Survived', errors='ignore')\n",
    "    test_ds['PassengerId'] = test_ds['PassengerId'].astype(int)\n",
    "\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        features.drop(columns='PassengerId'), labels, test_size=0.2, random_state=SEED, stratify=labels\n",
    "    )\n",
    "    return features, labels, X_tr, X_val, y_tr, y_val, test_ds\n",
    "\n",
    "features, labels, X_train, X_test, y_train, y_test, test_dataset = build_features_from_raw()\n",
    "\n",
    "def _prep_test_X(df):\n",
    "    cols_to_drop = [c for c in ['PassengerId','Survived'] if c in df.columns]\n",
    "    return df.drop(columns=cols_to_drop)\n",
    "\n",
    "def sigmoid(x): return 1.0/(1.0+np.exp(-x))\n",
    "def prob_to_logit(p, eps=1e-9):\n",
    "    p = np.clip(p, eps, 1-eps); return np.log(p/(1-p))\n",
    "def rand_log_uniform(a, b):  # return 10**u, u~U(a,b)\n",
    "    return float(10 ** np.random.uniform(a, b))\n",
    "\n",
    "def evaluate_and_submit(model_name, model, Xtr, ytr, Xva, yva, test_df, submit_stem, want_proba=True):\n",
    "    t0 = time.time(); model.fit(Xtr, ytr); train_secs = time.time()-t0\n",
    "    val_pred_hard = model.predict(Xva)\n",
    "    val_acc = accuracy_score(yva, val_pred_hard)\n",
    "    if want_proba and hasattr(model, \"predict_proba\"):\n",
    "        val_p = model.predict_proba(Xva)[:, list(model.classes_).index(1)]\n",
    "        test_p = model.predict_proba(_prep_test_X(test_df))[:, list(model.classes_).index(1)]\n",
    "    elif want_proba and hasattr(model, \"decision_function\"):\n",
    "        val_p = sigmoid(model.decision_function(Xva))\n",
    "        test_p = sigmoid(model.decision_function(_prep_test_X(test_df)))\n",
    "        if np.ndim(val_p)>1: val_p = val_p.ravel()\n",
    "        if np.ndim(test_p)>1: test_p = test_p.ravel()\n",
    "    else:\n",
    "        val_p = val_pred_hard.astype(float)\n",
    "        test_p = model.predict(_prep_test_X(test_df)).astype(float)\n",
    "    sub = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': (test_p>=0.5).astype(int)})\n",
    "    sub_path = f'./submission/{submit_stem}.csv'; sub.to_csv(sub_path, index=False)\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'ValAccuracy': float(val_acc),\n",
    "        'TrainSeconds': round(train_secs, 3),\n",
    "        'BestParams': getattr(model, 'get_params', lambda: {})(),\n",
    "        'SubmissionPath': sub_path,\n",
    "        'val_prob': val_p, 'test_prob': test_p,\n",
    "        'val_logit': prob_to_logit(val_p), 'test_logit': prob_to_logit(test_p)\n",
    "    }\n",
    "\n",
    "results = []\n",
    "members = []\n",
    "\n",
    "print(f\"[PLAN] DT={N_DT_SAMPLES}, RF(rand)={N_RF_SAMPLES}, SVM(total)={N_SVM_SAMPLES_PER_KERNEL*len(SVM_KERNELS)}, RF(BO)={BO_INIT_SAMPLES+BO_MAX_ITERS}, MLP={(N_MLP_SAMPLES if RUN_MLP and TORCH_OK else 0)}\")\n",
    "os.makedirs(\"./submission\", exist_ok=True)\n",
    "\n",
    "# ===== 병렬 평가용 워커 =====\n",
    "_GLOBAL = {}\n",
    "def _init_worker(Xtr, ytr, Xva, yva):\n",
    "    global _GLOBAL\n",
    "    _GLOBAL['Xtr'] = Xtr; _GLOBAL['ytr'] = ytr; _GLOBAL['Xva'] = Xva; _GLOBAL['yva'] = yva\n",
    "\n",
    "def _eval_dt(params):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    Xtr=_GLOBAL['Xtr']; ytr=_GLOBAL['ytr']; Xva=_GLOBAL['Xva']; yva=_GLOBAL['yva']\n",
    "    m = DecisionTreeClassifier(**params)\n",
    "    m.fit(Xtr, ytr)\n",
    "    acc = accuracy_score(yva, m.predict(Xva))\n",
    "    return acc, params\n",
    "\n",
    "def _eval_svm(params):\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    Xtr=_GLOBAL['Xtr']; ytr=_GLOBAL['ytr']; Xva=_GLOBAL['Xva']; yva=_GLOBAL['yva']\n",
    "    m = SVC(**params)\n",
    "    m.fit(Xtr, ytr)\n",
    "    acc = accuracy_score(yva, m.predict(Xva))\n",
    "    return acc, params\n",
    "\n",
    "# ---------- Decision Tree (Random Search, 멀티프로세스) ----------\n",
    "dt_param_list = []\n",
    "rng = np.random.default_rng(SEED)\n",
    "for _ in range(N_DT_SAMPLES):\n",
    "    max_depth = None if rng.random()<DT_USE_NONE_DEPTH_PROB else int(rng.integers(DT_MAX_DEPTH_RANGE[0], DT_MAX_DEPTH_RANGE[1]+1))\n",
    "    dt_param_list.append(dict(\n",
    "        random_state=SEED,\n",
    "        criterion=random.choice(DT_CRITERIA),\n",
    "        splitter=random.choice(DT_SPLITTERS),\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=int(rng.integers(DT_MIN_SPLIT_RANGE[0], DT_MIN_SPLIT_RANGE[1]+1)),\n",
    "        min_samples_leaf=int(rng.integers(DT_MIN_LEAF_RANGE[0], DT_MIN_LEAF_RANGE[1]+1))\n",
    "    ))\n",
    "best_dt_acc, best_dt_params = -1.0, None\n",
    "with ProcessPoolExecutor(max_workers=CPU, initializer=_init_worker, initargs=(X_train, y_train, X_test, y_test)) as ex:\n",
    "    futures = [ex.submit(_eval_dt, p) for p in dt_param_list]\n",
    "    for fut in TQDM(as_completed(futures), total=len(futures), desc=\"DT random search\"):\n",
    "        acc, params = fut.result()\n",
    "        if acc > best_dt_acc:\n",
    "            best_dt_acc, best_dt_params = acc, params\n",
    "if best_dt_params is not None:\n",
    "    r = evaluate_and_submit(\"DecisionTree\", DecisionTreeClassifier(**best_dt_params), X_train, y_train, X_test, y_test, test_dataset, \"dt_submission\")\n",
    "    r['BestParams']=best_dt_params; results.append(r); members.append(r)\n",
    "\n",
    "# ---------- Random Forest (Random Search, 내부 멀티코어 n_jobs=CPU) ----------\n",
    "best_rf_acc, best_rf_params = -1.0, None\n",
    "for _ in TQDM(range(N_RF_SAMPLES), desc=\"RF random search\", total=N_RF_SAMPLES):\n",
    "    max_features_choice = random.choice(RF_MAX_FEATURES_CHOICES)\n",
    "    if max_features_choice == \"float\":\n",
    "        max_features = float(np.random.uniform(*RF_MAX_FEATURES_FLOAT_RANGE))\n",
    "    else:\n",
    "        max_features = max_features_choice\n",
    "    params = dict(\n",
    "        random_state=SEED,\n",
    "        n_estimators=int(np.random.randint(RF_N_EST_RANGE[0], RF_N_EST_RANGE[1]+1)),\n",
    "        max_depth=int(np.random.randint(RF_MAX_DEPTH_RANGE[0], RF_MAX_DEPTH_RANGE[1]+1)),\n",
    "        min_samples_split=int(np.random.randint(RF_MIN_SPLIT_RANGE[0], RF_MIN_SPLIT_RANGE[1]+1)),\n",
    "        min_samples_leaf=int(np.random.randint(RF_MIN_LEAF_RANGE[0], RF_MIN_LEAF_RANGE[1]+1)),\n",
    "        max_features=max_features,\n",
    "        bootstrap=random.choice(RF_BOOTSTRAP_CHOICES),\n",
    "        class_weight=random.choice(RF_CLASS_WEIGHT_CHOICES),\n",
    "        n_jobs=CPU\n",
    "    )\n",
    "    m = RandomForestClassifier(**params)\n",
    "    m.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, m.predict(X_test))\n",
    "    if acc > best_rf_acc:\n",
    "        best_rf_acc, best_rf_params = acc, params\n",
    "if best_rf_params is not None:\n",
    "    r = evaluate_and_submit(\"RandomForest(Rand)\", RandomForestClassifier(**best_rf_params), X_train, y_train, X_test, y_test, test_dataset, \"rf_rand_submission\")\n",
    "    r['BestParams']=best_rf_params; results.append(r); members.append(r)\n",
    "\n",
    "# ---------- SVM (Random Search, 멀티프로세스; 탐색 probability=False, 최종만 True) ----------\n",
    "svm_param_list = []\n",
    "for kernel in SVM_KERNELS:\n",
    "    for _ in range(N_SVM_SAMPLES_PER_KERNEL):\n",
    "        C = rand_log_uniform(*SVM_C_LOG_RANGE)\n",
    "        if kernel == \"linear\":\n",
    "            params = dict(C=C, kernel='linear', probability=False, random_state=SEED)\n",
    "        elif kernel == \"rbf\":\n",
    "            gamma = rand_log_uniform(*SVM_GAMMA_LOG_RANGE)\n",
    "            params = dict(C=C, kernel='rbf', gamma=gamma, probability=False, random_state=SEED)\n",
    "        elif kernel == \"poly\":\n",
    "            gamma = rand_log_uniform(*SVM_GAMMA_LOG_RANGE)\n",
    "            degree = int(np.random.randint(SVM_DEGREE_RANGE[0], SVM_DEGREE_RANGE[1]+1))\n",
    "            coef0 = float(np.random.uniform(*SVM_COEF0_RANGE))\n",
    "            params = dict(C=C, kernel='poly', gamma=gamma, degree=degree, coef0=coef0, probability=False, random_state=SEED)\n",
    "        else:  # sigmoid\n",
    "            gamma = rand_log_uniform(*SVM_GAMMA_LOG_RANGE)\n",
    "            coef0 = float(np.random.uniform(*SVM_COEF0_RANGE))\n",
    "            params = dict(C=C, kernel='sigmoid', gamma=gamma, coef0=coef0, probability=False, random_state=SEED)\n",
    "        svm_param_list.append(params)\n",
    "\n",
    "best_svm_acc, best_svm_params = -1.0, None\n",
    "with ProcessPoolExecutor(max_workers=CPU, initializer=_init_worker, initargs=(X_train, y_train, X_test, y_test)) as ex:\n",
    "    futures = [ex.submit(_eval_svm, p) for p in svm_param_list]\n",
    "    for fut in TQDM(as_completed(futures), total=len(futures), desc=\"SVM random search\"):\n",
    "        acc, params = fut.result()\n",
    "        if acc > best_svm_acc:\n",
    "            best_svm_acc, best_svm_params = acc, params\n",
    "# 최종 베스트만 probability=True로 재학습하여 확률 획득\n",
    "if best_svm_params is not None:\n",
    "    best_svm_params_prob = dict(best_svm_params); best_svm_params_prob['probability'] = True\n",
    "    svc_best = SVC(**best_svm_params_prob)\n",
    "    r = evaluate_and_submit(\"SVM\", svc_best, X_train, y_train, X_test, y_test, test_dataset, \"svm_submission\")\n",
    "    r['BestParams']=best_svm_params_prob; results.append(r); members.append(r)\n",
    "\n",
    "# ---------- Random Forest (Bayesian Optimization; 순차, 내부 멀티코어) ----------\n",
    "def expected_improvement(mu, sigma, y_best, xi=0.01):\n",
    "    Z = (mu - y_best - xi) / (sigma + 1e-12)\n",
    "    return (mu - y_best - xi) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "\n",
    "def rf_make_from_vec(x):\n",
    "    ne  = int(np.clip(round(x[0]), BO_BOUNDS_LO[0], BO_BOUNDS_HI[0]))\n",
    "    md  = int(np.clip(round(x[1]), BO_BOUNDS_LO[1], BO_BOUNDS_HI[1]))\n",
    "    mss = int(np.clip(round(x[2]), BO_BOUNDS_LO[2], BO_BOUNDS_HI[2]))\n",
    "    mxf = float(np.clip(x[3],    BO_BOUNDS_LO[3], BO_BOUNDS_HI[3]))\n",
    "    return RandomForestClassifier(random_state=SEED, n_estimators=ne, max_depth=md, min_samples_split=mss,\n",
    "                                  max_features=mxf, n_jobs=CPU), {'n_estimators':ne,'max_depth':md,'min_samples_split':mss,'max_features':mxf}\n",
    "\n",
    "def rf_objective(x):\n",
    "    m, pars = rf_make_from_vec(x)\n",
    "    m.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, m.predict(X_test))\n",
    "    return acc, pars\n",
    "\n",
    "X_obs=[]; y_obs=[]; P_obs=[]\n",
    "for _ in TQDM(range(BO_INIT_SAMPLES), desc=\"RF(BO) init\", total=BO_INIT_SAMPLES):\n",
    "    x = BO_BOUNDS_LO + np.random.rand(4)*(BO_BOUNDS_HI-BO_BOUNDS_LO)\n",
    "    acc, pars = rf_objective(x)\n",
    "    X_obs.append(x); y_obs.append(acc); P_obs.append(pars)\n",
    "X_obs=np.array(X_obs); y_obs=np.array(y_obs)\n",
    "\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=CK(1.0,(1e-3,1e3))*Matern(length_scale=[400,10,5,0.2], length_scale_bounds=(1e-2,1e3), nu=2.5) + WhiteKernel(noise_level=1e-5, noise_level_bounds=(1e-8,1e-1)),\n",
    "    alpha=1e-6, normalize_y=True, random_state=SEED\n",
    ")\n",
    "\n",
    "best=float(np.max(y_obs)); no_imp=0\n",
    "for _ in TQDM(range(BO_MAX_ITERS), desc=\"RF(BO) iter\", total=BO_MAX_ITERS):\n",
    "    try: gp.fit(X_obs, y_obs)\n",
    "    except Exception: gp.fit(X_obs + 1e-6*np.random.randn(*X_obs.shape), y_obs)\n",
    "    cand = BO_BOUNDS_LO + np.random.rand(BO_N_CANDIDATES,4)*(BO_BOUNDS_HI-BO_BOUNDS_LO)\n",
    "    mu, sig = gp.predict(cand, return_std=True)\n",
    "    x_next = cand[int(np.argmax(expected_improvement(mu, sig, best, xi=0.01)))]\n",
    "    acc, pars = rf_objective(x_next)\n",
    "    X_obs=np.vstack([X_obs,x_next]); y_obs=np.append(y_obs, acc); P_obs.append(pars)\n",
    "    if acc > best + BO_IMPROVEMENT_TOL:\n",
    "        best=acc; no_imp=0\n",
    "    else:\n",
    "        no_imp+=1\n",
    "        if no_imp>=BO_EARLY_STOP_PATIENCE: break\n",
    "\n",
    "bp = P_obs[int(np.argmax(y_obs))]\n",
    "rf_bo = RandomForestClassifier(random_state=SEED, **bp, n_jobs=CPU)\n",
    "r = evaluate_and_submit(\"RandomForest(BO)\", rf_bo, X_train, y_train, X_test, y_test, test_dataset, \"rf_bo_submission\")\n",
    "r['BestParams']=bp; results.append(r); members.append(r)\n",
    "\n",
    "# ---------- MLP (Random Search; GPU 단일 프로세스) ----------\n",
    "if RUN_MLP and TORCH_OK:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, in_dim, hidden=128, dropout=0.2):\n",
    "            super().__init__()\n",
    "            self.f = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden), nn.ReLU(), nn.Dropout(dropout),\n",
    "                nn.Linear(hidden, hidden), nn.ReLU(), nn.Dropout(dropout),\n",
    "                nn.Linear(hidden, 1)  # logit\n",
    "            )\n",
    "        def forward(self, x): return self.f(x)\n",
    "    def train_mlp_collect(Xtr, ytr, Xva, yva, Xte, hidden, dropout, lr, epochs, batch, es_patience):\n",
    "        Xtr_t=torch.tensor(Xtr.values, dtype=torch.float32).to(device)\n",
    "        ytr_t=torch.tensor(ytr.values.reshape(-1,1), dtype=torch.float32).to(device)\n",
    "        Xva_t=torch.tensor(Xva.values, dtype=torch.float32).to(device)\n",
    "        Xte_t=torch.tensor(Xte.values, dtype=torch.float32).to(device)\n",
    "        m=MLP(Xtr.shape[1], hidden, dropout).to(device); opt=optim.Adam(m.parameters(), lr=lr); crit=nn.BCEWithLogitsLoss()\n",
    "        best_acc=-1; best_state=None; n=Xtr_t.size(0); no_imp=0\n",
    "        for ep in range(int(epochs)):\n",
    "            m.train(); perm=torch.randperm(n, device=device)\n",
    "            for i in range(0,n,int(batch)):\n",
    "                idx=perm[i:i+int(batch)]; xb=Xtr_t[idx]; yb=ytr_t[idx]\n",
    "                opt.zero_grad(); loss=crit(m(xb), yb); loss.backward(); opt.step()\n",
    "            m.eval()\n",
    "            with torch.no_grad():\n",
    "                val_logits=m(Xva_t).cpu().numpy().ravel(); val_pred=(val_logits>0).astype(int)\n",
    "                acc=accuracy_score(yva.values, val_pred)\n",
    "                if acc>best_acc: best_acc=acc; best_state={k:v.detach().cpu().clone() for k,v in m.state_dict().items()}; no_imp=0\n",
    "                else: no_imp+=1\n",
    "            if no_imp>=es_patience: break\n",
    "        if best_state is not None: m.load_state_dict({k:v.to(device) for k,v in best_state.items()})\n",
    "        m.eval()\n",
    "        with torch.no_grad():\n",
    "            val_logits=m(Xva_t).cpu().numpy().ravel()\n",
    "            test_logits=m(Xte_t).cpu().numpy().ravel()\n",
    "        return m, val_logits, test_logits, best_acc\n",
    "    best_mlp, best_mlp_acc, best_cfg, best_val_p, best_test_p, best_val_logit, best_test_logit = None, -1.0, None, None, None, None, None\n",
    "    XteX=_prep_test_X(test_dataset)\n",
    "    for _ in TQDM(range(N_MLP_SAMPLES), desc=\"MLP random search\", total=N_MLP_SAMPLES):\n",
    "        hidden = int(np.random.randint(MLP_HIDDEN_RANGE[0], MLP_HIDDEN_RANGE[1]+1))\n",
    "        dropout = float(np.random.uniform(*MLP_DROPOUT_RANGE))\n",
    "        lr = float(10 ** np.random.uniform(*MLP_LR_LOG_RANGE))\n",
    "        epochs = int(np.random.randint(MLP_EPOCHS_RANGE[0], MLP_EPOCHS_RANGE[1]+1))\n",
    "        batch = random.choice(MLP_BATCH_CHOICES)\n",
    "        cfg={'hidden':hidden,'dropout':dropout,'lr':lr,'epochs':epochs,'batch':batch,'es_patience':MLP_EARLY_STOP_PATIENCE}\n",
    "        m, val_logits, test_logits, acc = train_mlp_collect(X_train, y_train, X_test, y_test, XteX, **cfg)\n",
    "        if acc > best_mlp_acc:\n",
    "            best_mlp_acc, best_mlp, best_cfg = acc, m, cfg\n",
    "            best_val_logit, best_test_logit = val_logits, test_logits\n",
    "            best_val_p = 1/(1+np.exp(-val_logits)); best_test_p = 1/(1+np.exp(-test_logits))\n",
    "    if best_mlp is not None:\n",
    "        mlp_sub='./submission/mlp_submission.csv'\n",
    "        pd.DataFrame({'PassengerId': test_dataset['PassengerId'],'Survived':(best_test_p>=0.5).astype(int)}).to_csv(mlp_sub, index=False)\n",
    "        r={'Model':'MLP(FC, PyTorch, GPU)' if torch.cuda.is_available() else 'MLP(FC, PyTorch, CPU)',\n",
    "           'ValAccuracy':float(best_mlp_acc),'TrainSeconds':None,'BestParams':best_cfg,'SubmissionPath':mlp_sub,\n",
    "           'val_prob':best_val_p,'test_prob':best_test_p,'val_logit':best_val_logit,'test_logit':best_test_logit}\n",
    "        results.append(r); members.append(r)\n",
    "\n",
    "# ---------- Soft voting ensemble ----------\n",
    "names=[m['Model'] for m in members]\n",
    "w = np.array([1.0 if not ENSEMBLE_WEIGHTS or n not in ENSEMBLE_WEIGHTS else float(ENSEMBLE_WEIGHTS[n]) for n in names], float)\n",
    "w = w / (w.sum() if w.sum()!=0 else 1.0)\n",
    "val_stack = np.stack([m['val_prob'] for m in members], axis=1)\n",
    "test_stack= np.stack([m['test_prob'] for m in members], axis=1)\n",
    "val_ens = (val_stack * w.reshape(1,-1)).sum(axis=1)\n",
    "test_ens= (test_stack* w.reshape(1,-1)).sum(axis=1)\n",
    "val_ens_acc = accuracy_score(y_test, (val_ens>=0.5).astype(int))\n",
    "ens_path='./submission/ensemble_softvote_submission.csv'\n",
    "pd.DataFrame({'PassengerId': test_dataset['PassengerId'], 'Survived': (test_ens>=0.5).astype(int)}).to_csv(ens_path, index=False)\n",
    "\n",
    "# ---------- 결과 출력 ----------\n",
    "rows=[[r['Model'], f\"{r['ValAccuracy']:.4f}\", r['BestParams'], r['SubmissionPath']] for r in results]\n",
    "summary=pd.DataFrame(rows, columns=['Model','ValAcc','BestParams','CSV']).sort_values('ValAcc', ascending=False).reset_index(drop=True)\n",
    "print(\"\\n[Per-model results]\")\n",
    "print(summary.to_string(index=False))\n",
    "print(f\"\\n[Ensemble] members={names}\\nWeights={w.tolist()}\\nValAccuracy(soft vote)={val_ens_acc:.4f}\\nCSV={ens_path}\")\n",
    "summary.to_csv('./submission/model_eval_summary.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
